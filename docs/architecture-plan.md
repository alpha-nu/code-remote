# Remote Code Execution Engine - Architecture Plan

## Mission

Build a secure, scalable, cloud-agnostic remote code execution platform that allows users to write Python code in a web interface, execute it safely, and receive results along with AI-powered complexity analysis.

## Quick Links

> **Detailed Documentation:**
> - [Architecture Overview](architecture/overview.md) - System design and components
> - [Security Model](architecture/security.md) - Sandbox, import restrictions, resource limits
> - [Data Model](architecture/data-model.md) - DynamoDB schemas, API contracts
> - [Infrastructure](architecture/infrastructure.md) - Pulumi components, AWS resources
> - [Phase Documentation](phases/README.md) - Implementation phases

---

## Architecture Decision: Hybrid (Option 3)

**Managed Services + Self-hosted Kubernetes Execution**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MANAGED SERVICES + SELF-HOSTED EXECUTION                  â”‚
â”‚                                                                              â”‚
â”‚  API Layer: API Gateway (HTTP + WebSocket) + Lambda                         â”‚
â”‚  Execution: Self-hosted Kubernetes cluster with gVisor (Phase 11)           â”‚
â”‚  Data: Aurora PostgreSQL (CRUD) + Neo4j AuraDB (search) - Phase 9           â”‚
â”‚  Queue: SQS FIFO (Phase 10) âœ…                                               â”‚
â”‚  Real-time: WebSocket API Gateway (Phase 10) âœ…                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Hybrid:**
- Managed services for API/auth/queue reduce ops burden
- Self-hosted K8s for execution gives full security control
- gVisor provides kernel-level isolation
- Network policies block all egress from execution pods

---

## Technology Stack

| Component | Technology | Phase |
|-----------|------------|-------|
| Frontend | React 18 + Monaco Editor + AWS Amplify | 4 |
| API Layer | AWS API Gateway (HTTP) + Lambda | 3, 7 |
| Backend | FastAPI + Mangum | 1-3 |
| Execution | Lambda (current), K8s + gVisor (Phase 11) | 2, 11 |
| Queue | AWS SQS FIFO | 10 |
| Database | Aurora PostgreSQL (CRUD), Neo4j AuraDB (search) | 9 |
| Auth | AWS Cognito | 6 |
| LLM | Google Gemini API | 5 |
| Real-time | API Gateway WebSocket | 10 |
| IaC | Pulumi (Python) | 7 |
| CI/CD | GitHub Actions | 8 |

---

## Phase Progress

| Phase | Name | Status | Description |
|-------|------|--------|-------------|
| 1 | Backend Foundation | âœ… Complete | FastAPI project structure |
| 2 | Sandboxed Executor | âœ… Complete | Python runner with security |
| 3 | API Integration | âœ… Complete | Execution and analysis endpoints |
| 4 | Frontend | âœ… Complete | React + Monaco Editor |
| 5 | LLM Analysis | âœ… Complete | Gemini complexity analysis |
| 6 | Authentication | âœ… Complete | Cognito integration |
| 7 | Infrastructure | âœ… Complete | Pulumi AWS deployment |
| 8 | CI/CD | âœ… Complete | GitHub Actions pipeline |
| 9 | Persistence | ï¿½ In Progress | PostgreSQL + Neo4j hybrid |
| 10 | Real-Time Async | âœ… Complete | WebSocket, SQS FIFO |
| 11 | Kubernetes | ğŸ“‹ Planned | EKS + gVisor execution |

---

## Current Focus: Phase 9

**Persistence with Hybrid PostgreSQL + Neo4j**

Save and search code snippets with a hybrid relational + graph architecture.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Phase 9: Hybrid Persistence                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚        Lambda (FastAPI)         â”‚                                       â”‚
â”‚  â”‚  - CRUD â†’ PostgreSQL            â”‚                                       â”‚
â”‚  â”‚  - Search â†’ Neo4j               â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                  â”‚                                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚         â–¼                 â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Aurora     â”‚   â”‚  Neo4j       â”‚                                      â”‚
â”‚  â”‚  PostgreSQL  â”‚â”€â”€â–¶â”‚  AuraDB      â”‚  (CDC via EventBridge)               â”‚
â”‚  â”‚              â”‚   â”‚              â”‚                                      â”‚
â”‚  â”‚  â€¢ Users     â”‚   â”‚  â€¢ Embeddingsâ”‚                                      â”‚
â”‚  â”‚  â€¢ Snippets  â”‚   â”‚  â€¢ SIMILAR_TOâ”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â€¢ Patterns  â”‚                                      â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 9.1 (Current):** PostgreSQL foundation - Users + Snippets CRUD (backend only)
**Phase 9.2:** Neo4j + EventBridge CDC sync
**Phase 9.3:** Vector search with Gemini embeddings

**See:** [Phase Documentation](phases/README.md)

---

## Decisions Made

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Architecture | Hybrid | Balance of managed services + execution control |
| Build Approach | Incremental | Build one component at a time, verify each |
| Frontend | React + Monaco | Industry standard, VS Code's editor |
| Authentication | AWS Cognito | Native AWS integration, managed |
| LLM Provider | Google Gemini | API key auth only, no GCP setup |
| Execution Timeout | 30 seconds | Complex computations while limiting abuse |
| Initial Cloud | AWS | Mature ecosystem, Cognito integration |
| Real-time | WebSocket | Native browser support, bidirectional |

---

## Security Summary

**Execution Sandbox (4 Layers):**
1. **Import Whitelist** - Only safe modules (math, json, collections, etc.)
2. **Restricted Builtins** - No eval, exec, open, __import__, compile
3. **AST Validation** - Block dangerous patterns at parse time
4. **Resource Limits** - 256MB memory, 30s timeout, no network

**See:** [Security Documentation](architecture/security.md)

---

## Project Structure

```
code-remote/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/         # GitHub Actions CI/CD
â”œâ”€â”€ frontend/              # React + Monaco Editor
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/               # FastAPI routers, schemas, services
â”‚   â”œâ”€â”€ executor/          # Sandboxed Python runner
â”‚   â”œâ”€â”€ analyzer/          # Gemini LLM integration
â”‚   â””â”€â”€ common/            # Shared config, utilities
â”œâ”€â”€ infra/pulumi/          # Infrastructure as Code
â”œâ”€â”€ docs/                  # Architecture documentation
â”‚   â”œâ”€â”€ architecture/      # System design docs
â”‚   â””â”€â”€ phases/            # Phase implementation docs
â””â”€â”€ kubernetes/            # K8s manifests (Phase 11)
```

---

## Local Development

```bash
# Backend
cd backend && uvicorn api.main:app --reload --port 8000

# Frontend
cd frontend && npm run dev

# Full stack with Docker
docker-compose up -d
```

---

## Deployment

```bash
# Deploy infrastructure
cd infra/pulumi && pulumi up --stack dev

# Build and push API container
docker buildx build -t $ECR_URL:latest -f backend/Dockerfile.lambda backend/
docker push $ECR_URL:latest

# Update Lambda
aws lambda update-function-code --function-name $FUNC --image-uri $ECR_URL:latest
```

**See:** [Deployment Guide](DEPLOYMENT.md) | [Release Workflow](RELEASE_WORKFLOW.md)

---

## Status: APPROVED âœ…

Ready for Phase 10 implementation.


# Remote Code Execution Engine - Architecture Plan

## Mission
Build a secure, scalable, cloud-agnostic remote code execution platform that allows users to write Python code in a web interface, execute it safely, and receive results along with AI-powered complexity analysis.

---

## Architecture Options

### Option 1: Kubernetes-Native Architecture (Recommended)

**Best for:** Production-grade, cloud-agnostic, full control over execution environment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                   FRONTEND                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  React/Vue + Monaco Editor (VS Code's editor)                       â”‚    â”‚
â”‚  â”‚  - Code editing with Python syntax highlighting                      â”‚    â”‚
â”‚  â”‚  - Basic intellisense via Pyright WASM                              â”‚    â”‚
â”‚  â”‚  - WebSocket for real-time execution output                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              API GATEWAY LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Kong / Traefik / AWS API Gateway (abstracted via Pulumi)           â”‚    â”‚
â”‚  â”‚  - Rate limiting, Authentication (JWT/OAuth2)                        â”‚    â”‚
â”‚  â”‚  - Request validation, CORS                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXECUTION SERVICE   â”‚ â”‚  ANALYSIS SERVICE â”‚ â”‚     SESSION SERVICE       â”‚
â”‚   (FastAPI)           â”‚ â”‚  (FastAPI)        â”‚ â”‚     (FastAPI)             â”‚
â”‚                       â”‚ â”‚                   â”‚ â”‚                           â”‚
â”‚ - Receives code       â”‚ â”‚ - LLM integration â”‚ â”‚ - User session mgmt       â”‚
â”‚ - Validates input     â”‚ â”‚ - Complexity calc â”‚ â”‚ - Execution history       â”‚
â”‚ - Queues execution    â”‚ â”‚ - Code review     â”‚ â”‚ - Rate limit tracking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                       â”‚
            â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            MESSAGE QUEUE                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Redis Streams / RabbitMQ / AWS SQS (abstracted)                    â”‚    â”‚
â”‚  â”‚  - Execution job queue                                               â”‚    â”‚
â”‚  â”‚  - Result notification                                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXECUTION ORCHESTRATOR                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Kubernetes Job Controller (Custom Operator)                         â”‚    â”‚
â”‚  â”‚  - Spawns ephemeral pods for each execution                         â”‚    â”‚
â”‚  â”‚  - Resource limits (CPU, Memory, Time)                              â”‚    â”‚
â”‚  â”‚  - Network isolation (no egress by default)                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                      â”‚                                       â”‚
â”‚                                      â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  SANDBOXED EXECUTION POD                                            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  gVisor / Firecracker MicroVM                                 â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Hardened Python runtime                                    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - seccomp profiles                                           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Read-only filesystem                                       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - No network access                                          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Resource cgroups                                           â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA LAYER                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PostgreSQL      â”‚  â”‚  Redis           â”‚  â”‚  S3 / MinIO              â”‚   â”‚
â”‚  â”‚  - Execution logsâ”‚  â”‚  - Session cache â”‚  â”‚  - Code snapshots        â”‚   â”‚
â”‚  â”‚  - User data     â”‚  â”‚  - Rate limits   â”‚  â”‚  - Large outputs         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Kubernetes-Native:**
- **Cloud Agnostic:** Runs on EKS, GKE, AKS, or self-hosted
- **Isolation:** Each execution in its own pod with resource limits
- **Scalability:** HPA for services, cluster autoscaler for nodes
- **Security:** Network policies, pod security policies, gVisor runtime
- **Testability:** Kind/k3s for local development, same manifests everywhere

---

### Option 2: Serverless-First Architecture

**Best for:** Cost optimization, auto-scaling, minimal ops overhead

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 FRONTEND                                     â”‚
â”‚         Static site on CDN (CloudFront/CloudFlare)                          â”‚
â”‚         React + Monaco Editor                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVERLESS API LAYER                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  AWS Lambda / Google Cloud Run / Azure Functions                    â”‚    â”‚
â”‚  â”‚  (Abstracted via Pulumi's cloud-agnostic components)                â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚ /submit      â”‚  â”‚ /analyze     â”‚  â”‚ /status                  â”‚   â”‚    â”‚
â”‚  â”‚  â”‚ Lambda       â”‚  â”‚ Lambda       â”‚  â”‚ Lambda                   â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXECUTION via AWS Fargate / Cloud Run                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Ephemeral container per execution                                   â”‚    â”‚
â”‚  â”‚  - Timeout: 30 seconds max                                          â”‚    â”‚
â”‚  â”‚  - Memory: 512MB max                                                â”‚    â”‚
â”‚  â”‚  - VPC isolated, no internet                                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tradeoffs:**
- âœ… Zero infrastructure to manage
- âœ… Pay-per-execution pricing
- âŒ Cold start latency (1-3 seconds)
- âŒ Execution time limits (15 min Lambda, 60 min Cloud Run)
- âŒ Less control over sandbox security

---

### Option 3: Hybrid Architecture

**Best for:** Balance of control and simplicity

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MANAGED SERVICES + SELF-HOSTED EXECUTION                  â”‚
â”‚                                                                              â”‚
â”‚  API Layer: Managed (API Gateway + Lambda for routing)                      â”‚
â”‚  Execution: Self-hosted Kubernetes cluster with gVisor                      â”‚
â”‚  Data: Managed databases (RDS, ElastiCache)                                 â”‚
â”‚  Queue: Managed (SQS/SNS)                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Backend Component Design

### 1. API Service (FastAPI)

```python
# Structure
backend/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ execution.py     # POST /execute
â”‚   â”‚   â”œâ”€â”€ analysis.py      # POST /analyze
â”‚   â”‚   â””â”€â”€ health.py        # GET /health
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ execution.py     # Pydantic models
â”‚   â”‚   â””â”€â”€ analysis.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ executor.py      # Business logic
â”‚   â”‚   â”œâ”€â”€ analyzer.py      # LLM integration
â”‚   â”‚   â””â”€â”€ queue.py         # Message queue client
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ auth.py          # JWT validation
â”‚       â””â”€â”€ rate_limit.py    # Rate limiting
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â””â”€â”€ Dockerfile
```

**Why FastAPI:**
- Async-first for high concurrency
- Built-in OpenAPI documentation
- Pydantic for validation
- Easy to test with TestClient

### 2. Execution Sandbox

```python
# sandbox/
â”œâ”€â”€ executor/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ runner.py            # Main execution logic
â”‚   â”œâ”€â”€ security.py          # Import restrictions
â”‚   â”œâ”€â”€ resource_monitor.py  # CPU/Memory tracking
â”‚   â””â”€â”€ output_capture.py    # stdout/stderr capture
â”œâ”€â”€ Dockerfile               # Hardened Python image
â””â”€â”€ seccomp-profile.json     # System call restrictions
```

**Security Layers:**
1. **Container Level:** gVisor or Firecracker for kernel isolation
2. **Python Level:** RestrictedPython or custom AST validation
3. **Import Restrictions:** Whitelist-only imports (no `os`, `subprocess`, `socket`)
4. **Resource Limits:** cgroups for CPU (100ms max), Memory (128MB max)
5. **Timeout:** Hard kill after 10 seconds
6. **Network:** No egress allowed

### 3. LLM Analysis Service

```python
# analyzer/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ complexity.py            # Complexity analysis prompts
â”œâ”€â”€ llm_client.py            # Abstract LLM interface
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ openai.py
â”‚   â”œâ”€â”€ anthropic.py
â”‚   â””â”€â”€ local.py             # Ollama for testing
â””â”€â”€ prompts/
    â”œâ”€â”€ time_complexity.txt
    â””â”€â”€ space_complexity.txt
```

**LLM Integration Pattern:**
```python
class ComplexityAnalyzer:
    async def analyze(self, code: str) -> ComplexityResult:
        prompt = self._build_prompt(code)
        response = await self.llm_client.complete(prompt)
        return self._parse_response(response)
```

### 4. Infrastructure (Pulumi - Python)

```python
# infra/
â”œâ”€â”€ __main__.py              # Entry point
â”œâ”€â”€ config.py                # Environment config
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ networking.py        # VPC, subnets, security groups
â”‚   â”œâ”€â”€ kubernetes.py        # EKS/GKE cluster
â”‚   â”œâ”€â”€ database.py          # RDS/Cloud SQL
â”‚   â”œâ”€â”€ queue.py             # SQS/Pub-Sub
â”‚   â””â”€â”€ storage.py           # S3/GCS
â”œâ”€â”€ stacks/
â”‚   â”œâ”€â”€ dev.py
â”‚   â”œâ”€â”€ staging.py
â”‚   â””â”€â”€ prod.py
â””â”€â”€ Pulumi.yaml
```

**Cloud Agnostic Abstraction:**
```python
# components/database.py
from abc import ABC, abstractmethod

class DatabaseProvider(ABC):
    @abstractmethod
    def create_postgres(self, name: str, config: DBConfig) -> Database:
        pass

class AWSDatabase(DatabaseProvider):
    def create_postgres(self, name: str, config: DBConfig) -> Database:
        return aws.rds.Instance(name, ...)

class GCPDatabase(DatabaseProvider):
    def create_postgres(self, name: str, config: DBConfig) -> Database:
        return gcp.sql.DatabaseInstance(name, ...)
```

---

## Testing Strategy

### Unit Tests
```
pytest backend/tests/unit/ -v --cov=backend
```
- Mock external services (LLM, Queue, Database)
- Test business logic in isolation
- Target: 80%+ coverage

### Integration Tests
```
docker-compose -f docker-compose.test.yml up
pytest backend/tests/integration/ -v
```
- Real containers for dependencies (Postgres, Redis)
- Test API endpoints
- Test queue consumers

### E2E Tests
```
pytest backend/tests/e2e/ -v --env=staging
```
- Deploy to staging environment
- Full flow: Submit â†’ Execute â†’ Return result
- Security penetration tests

### Local Development
```
# Start all services locally
docker-compose up -d

# Run specific service
cd backend && uvicorn api.main:app --reload

# Run sandbox locally (with relaxed security)
docker run -it --rm executor:dev python runner.py
```

---

## Security Checklist

| Layer | Control | Implementation |
|-------|---------|----------------|
| Network | No egress | K8s NetworkPolicy deny-all egress |
| Container | Kernel isolation | gVisor runsc runtime |
| Container | Resource limits | CPU: 0.1, Memory: 128Mi, Timeout: 10s |
| Python | Import restrictions | AST analysis, whitelist only |
| Python | Dangerous functions | Block `eval`, `exec`, `compile` with literals only |
| API | Authentication | JWT with short expiry (15 min) |
| API | Rate limiting | 10 requests/minute per user |
| API | Input validation | Pydantic schemas, max code size 10KB |
| Data | Encryption | TLS in transit, AES-256 at rest |

---

## Recommended Technology Stack

| Component | Technology | Rationale |
|-----------|------------|-----------|
| Frontend | React + Monaco Editor | Industry standard, VS Code's editor |
| API Framework | FastAPI | Async, fast, great DX |
| Execution | Docker + gVisor | Security + portability |
| Orchestration | Kubernetes | Cloud agnostic |
| Queue | Redis Streams | Simple, fast, good for small messages |
| Database | PostgreSQL | Reliable, feature-rich |
| Cache | Redis | Session storage, rate limiting |
| IaC | Pulumi (Python) | Same language as backend |
| CI/CD | GitHub Actions | Integration with repo |
| Monitoring | Prometheus + Grafana | K8s native |
| LLM | OpenAI API (primary), Ollama (dev) | Quality + local testing |

---

## Project Structure (Complete)

```
code-remote/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ copilot-instructions.md
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â”œâ”€â”€ cd.yml
â”‚       â””â”€â”€ security.yml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ executor/
â”‚   â”œâ”€â”€ analyzer/
â”‚   â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ pulumi/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ docker/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api.md
â”‚   â””â”€â”€ security.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.test.yml
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

## Implementation Approach: Incremental Build (Option B) âœ…

We will build one component at a time, verifying each piece works before moving on. This approach ensures deep understanding of each part and catches integration issues early.

**Build Order:** Backend API â†’ Executor â†’ Frontend â†’ Infrastructure â†’ Integration

---

## Implementation Phases

### Phase 1: Backend Foundation âœ… COMPLETE
**Goal:** Working FastAPI service with local Docker development

| Step | Task | Status |
|------|------|--------|
| 1.1 | Create project structure and `pyproject.toml` | âœ… |
| 1.2 | FastAPI skeleton with `/health` endpoint | âœ… |
| 1.3 | Pydantic settings with `.env` support | âœ… |
| 1.4 | Docker Compose for local development | âœ… |
| 1.5 | Unit test setup with pytest | âœ… |
| 1.6 | CI pipeline (GitHub Actions) | âœ… |

---

### Phase 2: Sandboxed Executor âœ… COMPLETE
**Goal:** Secure Python code execution with resource limits

| Step | Task | Status |
|------|------|--------|
| 2.1 | Basic executor that runs Python code | âœ… |
| 2.2 | Stdout/stderr capture | âœ… |
| 2.3 | Timeout enforcement (30s max) | âœ… |
| 2.4 | Import restrictions (whitelist) | âœ… |
| 2.5 | Resource limits (memory) | âœ… (via container) |
| 2.6 | Executor Docker image | âœ… |
| 2.7 | Unit tests (60 tests, 80% coverage) | âœ… |

**Key Files:**
- `backend/executor/security.py` - Import whitelist, AST validation
- `backend/executor/runner.py` - Sandboxed execution with multiprocessing
| 2.6 | Executor Docker image | Run executor in container |
| 2.7 | Integration tests | Full execution flow tested |

**Deliverables:**
- `backend/executor/` with sandboxed runner
- `backend/executor/Dockerfile`
- Security whitelist configuration

---

### Phase 3: API Integration
**Goal:** Connect API to executor via queue

| Step | Task | Verification |
|------|------|--------------|
| 3.1 | Execution request/response schemas | Pydantic models validate |
| 3.2 | `/execute` endpoint (sync for now) | Submit code, get result |
| 3.3 | Redis Streams queue setup | Queue accepts messages |
| 3.4 | Async execution via queue | Submit â†’ poll â†’ get result |
| 3.5 | `/status/{id}` endpoint | Check execution status |
| 3.6 | Error handling and validation | Bad input returns proper errors |

**Deliverables:**
- `backend/api/routers/execution.py`
- `backend/api/services/executor_service.py`
- Queue integration with Redis

---

### Phase 4: Frontend
**Goal:** Working code editor that submits and displays results

| Step | Task | Verification |
|------|------|--------------|
| 4.1 | React + Vite project setup | `npm run dev` starts app |
| 4.2 | Monaco Editor integration | Editor renders with Python syntax |
| 4.3 | API client for execution | Submit button calls `/execute` |
| 4.4 | Result display panel | See stdout/stderr/errors |
| 4.5 | Loading states and error handling | UX for pending/error states |
| 4.6 | Basic styling | Clean, usable interface |

**Deliverables:**
- `frontend/` with React + Monaco
- Working end-to-end execution flow

---

### Phase 5: LLM Complexity Analysis
**Goal:** Gemini-powered code analysis

| Step | Task | Verification |
|------|------|--------------|
| 5.1 | Gemini provider with API key auth | Connection test passes |
| 5.2 | Complexity analysis prompt | Returns time/space complexity |
| 5.3 | `/analyze` endpoint | Submit code, get analysis |
| 5.4 | Frontend integration | Show complexity after execution |
| 5.5 | Prompt refinement | Accurate results on test cases |

**Deliverables:**
- `backend/analyzer/` with Gemini provider
- Complexity prompts in `backend/analyzer/prompts/`

---

### Phase 6: Authentication
**Goal:** Cognito-based user authentication

| Step | Task | Verification |
|------|------|--------------|
| 6.1 | Cognito user pool setup (Pulumi) | Pool created in AWS |
| 6.2 | JWT validation middleware | Protected endpoints require token |
| 6.3 | AWS Amplify frontend integration | Login/logout flow works |
| 6.4 | User context in execution | Executions tied to user |

**Deliverables:**
- `infra/pulumi/components/auth.py`
- `backend/api/middleware/auth.py`
- Frontend auth flow

---

### Phase 7: Infrastructure & Deployment
**Goal:** Production-ready AWS deployment

| Step | Task | Verification |
|------|------|--------------|
| 7.1 | Pulumi project structure | `pulumi preview` works |
| 7.2 | VPC and networking | Resources created |
| 7.3 | RDS PostgreSQL | Database accessible |
| 7.4 | SQS queues | Messages flow through |
| 7.5 | EKS cluster for executor | Pods run successfully |
| 7.6 | ECR for container images | Images push/pull |
| 7.7 | GitHub Actions deploy workflow | Push to release branch deploys |

**Deliverables:**
- `infra/pulumi/` complete
- `kubernetes/` manifests
- `.github/workflows/deploy.yml`

---

### Phase 8: Security Hardening
**Goal:** Production security posture

| Step | Task | Verification |
|------|------|--------------|
| 8.1 | gVisor runtime on executor pods | Pods use runsc |
| 8.2 | Network policies (no egress) | Executor can't reach internet |
| 8.3 | Secrets Manager integration | No secrets in code/config |
| 8.4 | Security scanning in CI | Vulnerabilities flagged |
| 8.5 | Rate limiting | Abuse prevented |

**Deliverables:**
- Hardened Kubernetes manifests
- Security scanning workflow

---

## Decisions Made âœ…

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Architecture | **Option 3: Hybrid** | Balance of managed services + execution control |
| **Build Approach** | **Option B: Incremental** | Build one component at a time, verify each works |
| Frontend | **React + Monaco Editor** | Industry standard, VS Code's editor |
| Authentication | **AWS Cognito** | Native AWS integration, managed service |
| LLM Provider | **Google Gemini** | API key auth only, no GCP setup required |
| Execution Timeout | **30 seconds** | Allows complex computations while limiting abuse |
| Initial Cloud | **AWS** | Mature ecosystem, Cognito integration |

---

## Final Technology Stack

| Component | Technology |
|-----------|------------|
| Frontend | React 18 + Monaco Editor + AWS Amplify |
| API Layer | AWS API Gateway + Lambda (routing) |
| Backend Services | FastAPI (containerized on ECS Fargate) |
| Execution | Self-hosted K8s cluster with gVisor |
| Queue | AWS SQS |
| Database | AWS RDS PostgreSQL |
| Cache | AWS ElastiCache Redis |
| Auth | AWS Cognito |
| LLM | Google Gemini API |
| IaC | Pulumi (Python) |
| CI/CD | GitHub Actions |

---

## Status: APPROVED âœ…

- `.github/copilot-instructions.md` has been generated
- `documentation/RELEASE_WORKFLOW.md` documents deployment workflow
- **Current Phase: 1 - Backend Foundation** (Ready to start)

---

## Phase 9: Persistence & Code Snippets (IN PROGRESS)

**Goal:** Allow users to save, organize, and search code snippets with AI-powered semantic search

**Decision:** Hybrid Architecture - PostgreSQL (CRUD) + Neo4j (Vector Search)

### Feature Overview

Users can:
- Save code snippets with name, description, and analysis results
- Star favorite snippets
- Semantic search: "find my recursive snippets"

### Database Options Analysis

#### Option 1: PostgreSQL + pgvector (Recommended)
**Aurora Serverless v2 or RDS**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL + pgvector                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Single DB for structured data AND vector embeddings      â”‚
â”‚  âœ“ Relational model fits users â†’ snippets naturally         â”‚
â”‚  âœ“ pgvector: mature, supports cosine/L2/inner product       â”‚
â”‚  âœ“ Hybrid search: combine WHERE clauses with vector search  â”‚
â”‚  âœ“ Aurora Serverless v2 scales to near-zero                 â”‚
â”‚  âœ— Not truly serverless (min capacity units)                â”‚
â”‚  âœ— Cold starts possible                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Schema:**
```sql
-- Users (from Cognito, cached locally)
CREATE TABLE users (
    id UUID PRIMARY KEY,
    cognito_sub VARCHAR(255) UNIQUE NOT NULL,
    email VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Snippets
CREATE TABLE snippets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    code TEXT NOT NULL,
    language VARCHAR(50) DEFAULT 'python',
    
    -- Analysis results (nullable until analyzed)
    time_complexity VARCHAR(50),
    space_complexity VARCHAR(50),
    explanation TEXT,
    suggestions TEXT,
    analyzed_at TIMESTAMPTZ,
    model_used VARCHAR(100),
    
    -- Metadata
    is_starred BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Vector embedding for semantic search (768 dims for Gemini)
    embedding vector(768)
);

-- Indexes
CREATE INDEX idx_snippets_user_id ON snippets(user_id);
CREATE INDEX idx_snippets_starred ON snippets(user_id, is_starred) WHERE is_starred = TRUE;
CREATE INDEX idx_snippets_embedding ON snippets USING ivfflat (embedding vector_cosine_ops);
```

---

#### Option 2: DynamoDB + OpenSearch Serverless
**Fully Serverless, AWS-Native**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DynamoDB + OpenSearch Serverless                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Truly serverless, scales to zero                         â”‚
â”‚  âœ“ Native AWS integration                                   â”‚
â”‚  âœ“ OpenSearch has k-NN vector search                        â”‚
â”‚  âœ— Two services to manage and sync                          â”‚
â”‚  âœ— OpenSearch Serverless has minimum cost (~$700/mo)        â”‚
â”‚  âœ— Complex for relational queries (denormalization needed)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Schema (Single-Table Design):**
```
PK                      SK                      Attributes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USER#<userId>           PROFILE                 email, createdAt
USER#<userId>           SNIPPET#<snippetId>     name, code, description, ...
USER#<userId>           STARRED#<snippetId>     (GSI for starred queries)
```

---

#### Option 3: MongoDB Atlas
**Flexible Schema + Built-in Vector Search**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB Atlas                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Atlas Vector Search built-in (no separate service)       â”‚
â”‚  âœ“ Flexible schema for evolving analysis results            â”‚
â”‚  âœ“ Good aggregation pipeline for complex queries            â”‚
â”‚  âœ“ Serverless tier available                                â”‚
â”‚  âœ— Outside AWS ecosystem (adds latency)                     â”‚
â”‚  âœ— Another platform/credentials to manage                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Option 4: Supabase (PostgreSQL + pgvector)
**Managed PostgreSQL with Extras**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Managed Postgres + pgvector                              â”‚
â”‚  âœ“ Real-time subscriptions (future collab features)         â”‚
â”‚  âœ“ Built-in auth (could replace Cognito eventually)         â”‚
â”‚  âœ“ Edge functions                                           â”‚
â”‚  âœ— Outside AWS (latency, another platform)                  â”‚
â”‚  âœ— Vendor lock-in to Supabase specifics                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Option 5: Neo4j AuraDB (Graph Database)
**Native Graph with Vector Search**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Neo4j AuraDB + Vector Index                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Native graph traversals for relationship queries                     â”‚
â”‚  âœ“ Neo4j 5.x has native vector search (vector indexes)                  â”‚
â”‚  âœ“ Cypher query language is expressive and readable                     â”‚
â”‚  âœ“ AuraDB Free tier available (good for dev)                            â”‚
â”‚  âœ“ Excellent for "similar to", "related to", recommendations            â”‚
â”‚  âœ“ Can model code patterns as a knowledge graph                         â”‚
â”‚  âœ— Outside AWS ecosystem (adds latency, another platform)               â”‚
â”‚  âœ— Overkill if relationships stay simple                                â”‚
â”‚  âœ— Learning curve for Cypher if team is SQL-focused                     â”‚
â”‚  âœ— AuraDB Pro pricing can get expensive                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Graph Databases Could Be Valuable:**

For semantic search and future features, graphs excel at modeling relationships:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Potential Relationships in Code Snippets                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚ User â”‚â”€â”€ownsâ”€â”€â–¶â”‚ Snippet â”‚â”€â”€usesâ”€â”€â–¶â”‚ Pattern â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚      â”‚                  â”‚                   â”‚                           â”‚
â”‚      â”‚ starred          â”‚ similar_to        â”‚ related_to               â”‚
â”‚      â–¼                  â–¼                   â–¼                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚ Snippet â”‚      â”‚ Snippet â”‚         â”‚ Pattern â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â”‚                                               â”‚
â”‚                         â”‚ has_complexity                                â”‚
â”‚                         â–¼                                               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚                    â”‚ O(n log n)â”‚                                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Graph Schema:**

```cypher
// Node types
(:User {id, cognito_sub, email, created_at})
(:Snippet {id, name, description, code, language, time_complexity, 
           space_complexity, embedding, created_at, updated_at})
(:Pattern {name, description})  -- e.g., "recursion", "memoization", "two-pointer"
(:Complexity {notation, category})  -- e.g., "O(n)", "linear"
(:Tag {name})

// Relationship types
(:User)-[:OWNS]->(:Snippet)
(:User)-[:STARRED {starred_at}]->(:Snippet)
(:Snippet)-[:SIMILAR_TO {score}]->(:Snippet)  -- computed from embeddings
(:Snippet)-[:USES]->(:Pattern)  -- extracted by LLM during analysis
(:Snippet)-[:HAS_TIME_COMPLEXITY]->(:Complexity)
(:Snippet)-[:HAS_SPACE_COMPLEXITY]->(:Complexity)
(:Snippet)-[:TAGGED_WITH]->(:Tag)
(:Pattern)-[:RELATED_TO]->(:Pattern)  -- e.g., recursion â†’ memoization
```

**Powerful Graph Queries:**

```cypher
-- "Find snippets similar to my starred snippets"
MATCH (me:User {id: $userId})-[:STARRED]->(s:Snippet)-[:SIMILAR_TO*1..2]->(related:Snippet)
WHERE NOT (me)-[:OWNS]->(related)
RETURN related

-- "Find all recursive patterns I've used"
MATCH (me:User)-[:OWNS]->(s:Snippet)-[:USES]->(p:Pattern {name: 'recursion'})
RETURN s, p

-- "Recommend snippets based on users with similar patterns"
MATCH (me:User)-[:OWNS]->(:Snippet)-[:USES]->(p:Pattern)<-[:USES]-(:Snippet)<-[:OWNS]-(other:User)
MATCH (other)-[:STARRED]->(recommended:Snippet)
WHERE NOT (me)-[:OWNS]->(recommended)
RETURN recommended, count(*) AS score ORDER BY score DESC
```

**Hybrid Search (Vector + Graph):**

```cypher
// Semantic search with graph context
CALL db.index.vector.queryNodes('snippet-embeddings', 10, $query_embedding)
YIELD node AS snippet, score
MATCH (user:User {id: $userId})-[:OWNS]->(snippet)
OPTIONAL MATCH (snippet)-[:USES]->(pattern:Pattern)
RETURN snippet, collect(pattern.name) AS patterns, score
ORDER BY score DESC
```

---

#### Option 6: Amazon Neptune (AWS-Native Graph)
**AWS-Managed Graph Database**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Amazon Neptune                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ AWS-native, VPC integration, IAM auth                                â”‚
â”‚  âœ“ Supports both Gremlin and SPARQL                                     â”‚
â”‚  âœ“ Neptune Analytics has vector similarity (preview)                    â”‚
â”‚  âœ“ Serverless option available                                          â”‚
â”‚  âœ— More expensive than Neo4j AuraDB Free                                â”‚
â”‚  âœ— Gremlin syntax is verbose compared to Cypher                         â”‚
â”‚  âœ— Vector search less mature than pgvector or Neo4j                     â”‚
â”‚  âœ— Minimum ~$0.10/hour even for serverless                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Graph vs. Relational Comparison

| Feature | Relational (pgvector) | Graph (Neo4j) |
|---------|----------------------|---------------|
| **User â†’ Snippet CRUD** | âœ… Simple, fast | âš ï¸ Works but overkill |
| **Semantic search** | âœ… pgvector excellent | âœ… Neo4j vector indexes |
| **"Similar snippets"** | âš ï¸ Requires joins | âœ… Native traversal |
| **Pattern knowledge graph** | âŒ Complex JOINs | âœ… Natural fit |
| **Recommendations** | âš ï¸ Complex queries | âœ… Graph algorithms built-in |
| **Multi-hop queries** | âŒ Recursive CTEs | âœ… Simple traversals |
| **AWS integration** | âœ… Aurora native | âš ï¸ Neptune or external |
| **Operational simplicity** | âœ… Familiar SQL | âš ï¸ New paradigm |

---

### Option 7: Hybrid Approach (PostgreSQL + Neo4j)
**Best of Both Worlds**

For projects that need strong CRUD with eventual graph capabilities:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Hybrid Architecture                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      Lambda (FastAPI)                            â”‚   â”‚
â”‚  â”‚  - CRUD operations â†’ PostgreSQL                                  â”‚   â”‚
â”‚  â”‚  - Recommendations â†’ Neo4j                                       â”‚   â”‚
â”‚  â”‚  - Semantic search â†’ PostgreSQL (pgvector)                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                           â”‚                        â”‚
â”‚                    â–¼                           â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Aurora PostgreSQL      â”‚    â”‚   Neo4j AuraDB                   â”‚  â”‚
â”‚  â”‚   + pgvector             â”‚â”€â”€â”€â–¶â”‚   (sync via CDC/events)          â”‚  â”‚
â”‚  â”‚                          â”‚    â”‚                                  â”‚  â”‚
â”‚  â”‚   â€¢ Users table          â”‚    â”‚   â€¢ Pattern knowledge graph      â”‚  â”‚
â”‚  â”‚   â€¢ Snippets table       â”‚    â”‚   â€¢ Similarity relationships     â”‚  â”‚
â”‚  â”‚   â€¢ Vector embeddings    â”‚    â”‚   â€¢ Recommendations engine       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phased Implementation:**
1. **Phase 1:** PostgreSQL handles CRUD + semantic search (simpler, cheaper)
2. **Phase 2:** If pattern detection is added (LLM extracts "uses recursion"), add Neo4j for the knowledge graph
3. **Phase 3:** Use Neo4j for recommendations ("users who wrote similar code also liked...")

---

#### CDC/Events Synchronization Deep Dive

**What is CDC (Change Data Capture)?**

CDC captures row-level changes (INSERT, UPDATE, DELETE) from PostgreSQL and streams them to other systems. For the hybrid approach, we sync snippet data from PostgreSQL (source of truth) to Neo4j (graph layer).

**Synchronization Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL â†’ Neo4j Sync Options                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  Option A: AWS DMS (Database Migration Service)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Aurora     â”‚â”€â”€â”€â–¶â”‚   AWS DMS   â”‚â”€â”€â”€â–¶â”‚   Kinesis    â”‚â”€â”€â”€â–¶â”‚   Lambda    â”‚    â”‚
â”‚  â”‚  PostgreSQL  â”‚    â”‚  (CDC task) â”‚    â”‚   Stream     â”‚    â”‚  (writer)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                                          â”‚           â”‚
â”‚         â”‚ logical replication                                      â”‚ Cypher    â”‚
â”‚         â”‚ (wal2json)                                               â–¼           â”‚
â”‚         â”‚                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚         â”‚                                                   â”‚   Neo4j     â”‚    â”‚
â”‚         â””â”€ Snapshots table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   AuraDB    â”‚    â”‚
â”‚                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  Option B: Application-Level Events (Simpler for MVP)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Lambda     â”‚â”€â”€â”€â–¶â”‚  EventBridgeâ”‚â”€â”€â”€â–¶â”‚   Lambda     â”‚â”€â”€â”€â–¶â”‚   Neo4j     â”‚    â”‚
â”‚  â”‚  (API CRUD)  â”‚    â”‚  (event bus)â”‚    â”‚  (sync fn)   â”‚    â”‚   AuraDB    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                                                       â”‚
â”‚         â”‚ writes                                                                â”‚
â”‚         â–¼                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚
â”‚  â”‚   Aurora     â”‚                                                               â”‚
â”‚  â”‚  PostgreSQL  â”‚                                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚
â”‚                                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  Option C: Debezium (Open Source CDC)                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Aurora     â”‚â”€â”€â”€â–¶â”‚  Debezium   â”‚â”€â”€â”€â–¶â”‚    Kafka     â”‚â”€â”€â”€â–¶â”‚   Kafka     â”‚    â”‚
â”‚  â”‚  PostgreSQL  â”‚    â”‚  Connector  â”‚    â”‚   (MSK)      â”‚    â”‚  Consumer   â”‚â”€â”€â”€â”€â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚      â”‚
â”‚                                                                          â–¼      â”‚
â”‚                                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                                                   â”‚  Neo4j  â”‚   â”‚
â”‚                                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recommended: Option B (Application-Level Events)**

For our scale and simplicity goals, application-level events via EventBridge is the best fit:

```python
# backend/api/services/snippet_service.py

from backend.common.events import event_bus

class SnippetService:
    async def create_snippet(self, user_id: str, data: SnippetCreate) -> Snippet:
        # 1. Write to PostgreSQL (source of truth)
        snippet = await self.repo.create(user_id, data)
        
        # 2. Emit event for graph sync (async, non-blocking)
        await event_bus.publish(
            event_type="snippet.created",
            payload={
                "snippet_id": str(snippet.id),
                "user_id": user_id,
                "patterns": snippet.detected_patterns,  # LLM-extracted
                "complexity": snippet.time_complexity,
                "embedding": snippet.embedding.tolist(),
            }
        )
        
        return snippet
    
    async def update_snippet(self, snippet_id: str, data: SnippetUpdate) -> Snippet:
        snippet = await self.repo.update(snippet_id, data)
        await event_bus.publish("snippet.updated", {...})
        return snippet
    
    async def delete_snippet(self, snippet_id: str) -> None:
        await self.repo.delete(snippet_id)
        await event_bus.publish("snippet.deleted", {"snippet_id": snippet_id})
```

**EventBridge Configuration (Pulumi):**

```python
# infra/pulumi/components/events.py

import pulumi_aws as aws

def create_snippet_event_bus(env: str):
    # Custom event bus for snippet events
    event_bus = aws.cloudwatch.EventBus(
        f"{env}-snippet-events",
        name=f"code-remote-{env}-snippets"
    )
    
    # Rule to route snippet events to Neo4j sync Lambda
    neo4j_sync_rule = aws.cloudwatch.EventRule(
        f"{env}-neo4j-sync-rule",
        event_bus_name=event_bus.name,
        event_pattern=json.dumps({
            "source": ["code-remote.snippets"],
            "detail-type": ["snippet.created", "snippet.updated", "snippet.deleted"]
        })
    )
    
    # Target: Lambda function for Neo4j sync
    aws.cloudwatch.EventTarget(
        f"{env}-neo4j-sync-target",
        rule=neo4j_sync_rule.name,
        event_bus_name=event_bus.name,
        arn=neo4j_sync_lambda.arn
    )
    
    return event_bus
```

**Neo4j Sync Lambda:**

```python
# backend/api/handlers/neo4j_sync.py

from neo4j import AsyncGraphDatabase

async def handler(event, context):
    """Process EventBridge events and sync to Neo4j."""
    
    driver = AsyncGraphDatabase.driver(
        settings.NEO4J_URI,
        auth=(settings.NEO4J_USER, settings.NEO4J_PASSWORD)
    )
    
    async with driver.session() as session:
        detail_type = event["detail-type"]
        payload = event["detail"]
        
        if detail_type == "snippet.created":
            await session.run("""
                MERGE (u:User {id: $user_id})
                CREATE (s:Snippet {
                    id: $snippet_id,
                    embedding: $embedding
                })
                CREATE (u)-[:OWNS]->(s)
                
                // Create pattern relationships
                WITH s
                UNWIND $patterns AS pattern_name
                MERGE (p:Pattern {name: pattern_name})
                CREATE (s)-[:USES]->(p)
                
                // Create complexity node
                WITH s
                MERGE (c:Complexity {notation: $complexity})
                CREATE (s)-[:HAS_COMPLEXITY]->(c)
            """, {
                "user_id": payload["user_id"],
                "snippet_id": payload["snippet_id"],
                "embedding": payload["embedding"],
                "patterns": payload.get("patterns", []),
                "complexity": payload.get("complexity", "unknown")
            })
            
        elif detail_type == "snippet.deleted":
            await session.run("""
                MATCH (s:Snippet {id: $snippet_id})
                DETACH DELETE s
            """, {"snippet_id": payload["snippet_id"]})
            
        elif detail_type == "snippet.updated":
            # Update snippet properties and relationships
            await session.run("""
                MATCH (s:Snippet {id: $snippet_id})
                SET s.embedding = $embedding
                
                // Update patterns (delete old, create new)
                WITH s
                OPTIONAL MATCH (s)-[r:USES]->(:Pattern)
                DELETE r
                
                WITH s
                UNWIND $patterns AS pattern_name
                MERGE (p:Pattern {name: pattern_name})
                CREATE (s)-[:USES]->(p)
            """, payload)
    
    await driver.close()
```

**Similarity Computation (Async Job):**

The `SIMILAR_TO` relationships between snippets can be computed periodically or on-demand:

```python
# backend/jobs/compute_similarity.py

async def compute_snippet_similarities(user_id: str):
    """Compute and store similarity relationships in Neo4j."""
    
    async with neo4j_driver.session() as session:
        # Find similar snippets using vector index
        await session.run("""
            // For each snippet owned by user
            MATCH (u:User {id: $user_id})-[:OWNS]->(s:Snippet)
            WHERE s.embedding IS NOT NULL
            
            // Find similar snippets (vector search)
            CALL db.index.vector.queryNodes(
                'snippet-embeddings', 
                5,  // top 5 similar
                s.embedding
            ) YIELD node AS similar, score
            
            // Don't link to self
            WHERE similar.id <> s.id AND score > 0.8
            
            // Create or update similarity relationship
            MERGE (s)-[r:SIMILAR_TO]->(similar)
            SET r.score = score, r.computed_at = datetime()
        """, {"user_id": user_id})
```

**Event Flow Diagram:**

```
User saves snippet
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POST /snippets   â”‚
â”‚  (Lambda/FastAPI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                              â”‚
          â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL         â”‚      â”‚  EventBridge        â”‚
â”‚  (INSERT snippet)   â”‚      â”‚  (emit event)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  Neo4j Sync Lambda  â”‚
                             â”‚  (process event)    â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  Neo4j AuraDB       â”‚
                             â”‚  (CREATE nodes,     â”‚
                             â”‚   relationships)    â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Consistency Considerations:**

| Concern | Solution |
|---------|----------|
| **Event ordering** | EventBridge preserves order per partition |
| **Duplicate events** | Make Neo4j operations idempotent (MERGE) |
| **Failed sync** | DLQ + retry with exponential backoff |
| **Initial sync** | One-time bulk sync job on Neo4j addition |
| **Data drift** | Periodic reconciliation job |

**Reconciliation Job (Weekly):**

```python
# backend/jobs/reconcile_graph.py

async def reconcile_postgres_neo4j():
    """Ensure Neo4j graph matches PostgreSQL source of truth."""
    
    # Get all snippet IDs from PostgreSQL
    pg_snippets = await pg_repo.get_all_snippet_ids()
    
    # Get all snippet IDs from Neo4j
    async with neo4j_driver.session() as session:
        result = await session.run("MATCH (s:Snippet) RETURN s.id AS id")
        neo4j_snippets = {r["id"] async for r in result}
    
    # Find orphaned nodes in Neo4j (deleted in PG)
    orphaned = neo4j_snippets - set(pg_snippets)
    if orphaned:
        await session.run(
            "UNWIND $ids AS id MATCH (s:Snippet {id: id}) DETACH DELETE s",
            {"ids": list(orphaned)}
        )
    
    # Find missing nodes in Neo4j (need sync)
    missing = set(pg_snippets) - neo4j_snippets
    if missing:
        for snippet_id in missing:
            snippet = await pg_repo.get(snippet_id)
            await sync_snippet_to_neo4j(snippet)
```

**Cost Estimate (with Neo4j sync):**

| Component | Monthly Cost |
|-----------|-------------|
| Aurora PostgreSQL (0.5 ACU avg) | ~$45 |
| EventBridge (1M events) | ~$1 |
| Lambda (sync function) | ~$5 |
| Neo4j AuraDB Free | $0 |
| **Total** | **~$51/mo** |

*Note: Neo4j AuraDB Free tier includes 200K nodes, 400K relationships - plenty for MVP*

---

### Recommendation: **PostgreSQL + pgvector on Aurora Serverless v2**

**Why:**

| Factor | PostgreSQL + pgvector |
|--------|----------------------|
| **Data Model** | Relational fits perfectly (users â†’ snippets) |
| **Vector Search** | pgvector in same DB, hybrid queries |
| **AWS Integration** | Aurora Serverless v2, IAM auth, VPC |
| **Cost** | ~$0.12/ACU-hour, scales down when idle |
| **Complexity** | Single database, familiar SQL |
| **Future-proof** | Can add full-text search, JSON columns |

**Hybrid Search Query Example:**
```sql
-- "Find my starred recursive snippets"
SELECT id, name, description, code,
       1 - (embedding <=> $query_embedding) AS similarity
FROM snippets
WHERE user_id = $user_id
  AND is_starred = TRUE
ORDER BY embedding <=> $query_embedding
LIMIT 10;
```

**Why Not Start with Graph?**
- MVP (save/search snippets) doesn't need graph traversals
- PostgreSQL is operationally simpler
- Can add Neo4j later without migrating (it's additive)

**Why Keep Graph in Mind?**
- The "semantic search" vision ("find my recursive snippets") is exactly what graphs excel at when combined with a pattern knowledge graph
- Future features like "suggest similar code" or "learn from others' patterns" would benefit enormously

---

### Embedding Strategy

For semantic search, embeddings are generated when snippets are saved:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Code Snippet   â”‚â”€â”€â”€â”€â–¶â”‚  Embedding API   â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚
â”‚  + Description  â”‚     â”‚  (Gemini/OpenAI) â”‚     â”‚  pgvector   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        768/1536 dims
```

**Options:**
1. **Gemini Embedding** - `text-embedding-004` (768 dims) - already have API key
2. **OpenAI** - `text-embedding-3-small` (1536 dims)
3. **Self-hosted** - Sentence Transformers (free, but need compute)

---

### Proposed Architecture with Persistence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Code Remote                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Frontend â”‚â”€â”€â”€â–¶â”‚ API Gateway  â”‚â”€â”€â”€â–¶â”‚ Lambda (FastAPI)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - /snippets CRUD       â”‚  â”‚
â”‚                                      â”‚  - /snippets/search     â”‚  â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                  â”‚                â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                         â”‚                        â”‚            â”‚   â”‚
â”‚                         â–¼                        â–¼            â”‚   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚              â”‚ Aurora Postgres â”‚    â”‚ Gemini Embedding    â”‚   â”‚   â”‚
â”‚              â”‚ + pgvector      â”‚â—€â”€â”€â”€â”‚ API                 â”‚   â”‚   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚                                                               â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Implementation Steps (Phase 9)

#### Phase 9.1: PostgreSQL Foundation (Backend Only)
| Step | Task | Details |
|------|------|---------|
| 9.1.1 | Pulumi: Aurora Serverless v2 | VPC, subnet groups, security groups |
| 9.1.2 | Database migrations | Alembic setup, initial schema |
| 9.1.3 | SQLAlchemy models | User, Snippet (no vector column yet) |
| 9.1.4 | User sync service | Sync Cognito users to PostgreSQL |
| 9.1.5 | Snippet CRUD endpoints | POST/GET/PUT/DELETE /snippets |

#### Phase 9.2: Neo4j Graph Layer + CDC Sync
| Step | Task | Details |
|------|------|---------|
| 9.2.1 | Neo4j AuraDB setup | Free tier, get connection credentials |
| 9.2.2 | EventBridge event bus | Route snippet events to sync Lambda |
| 9.2.3 | Sync Lambda | PG â†’ Neo4j on create/update/delete |
| 9.2.4 | Graph schema | User, Snippet, Pattern nodes |
| 9.2.5 | Reconciliation job | Weekly sync validation |

#### Phase 9.3: Vector Search
| Step | Task | Details |
|------|------|---------|
| 9.3.1 | Gemini embedding service | Generate embeddings on snippet save |
| 9.3.2 | Store embeddings in Neo4j | Vector index on Snippet nodes |
| 9.3.3 | Semantic search endpoint | GET /snippets/search?q= |
| 9.3.4 | SIMILAR_TO computation | Async job for similarity relationships |

#### Phase 9.4: Pattern Knowledge Graph (Future)
| Step | Task | Details |
|------|------|---------|
| 9.4.1 | LLM pattern extraction | Detect patterns during analysis |
| 9.4.2 | Pattern nodes | "recursion", "memoization", etc. |
| 9.4.3 | Advanced queries | "Find my recursive snippets" |

**Status:** âœ… APPROVED - Phase 9.1 starting (Backend Only)

**Decisions Made:**
- PostgreSQL: Aurora Serverless v2 (source of truth for CRUD)
- Graph DB: Neo4j AuraDB (free tier, Cypher queries)
- CDC: EventBridge (app-level events, simpler than DMS)
- Phasing: PostgreSQL first, Neo4j added in 9.2

---

## Phase 10: Async Execution & Event-Driven Architecture (âœ… COMPLETE)

**Goal:** Replace synchronous execution with robust async job processing using SQS, enabling retries, better UX, and laying groundwork for future eventing needs.

**Implementation:** Stateless WebSocket design - no DynamoDB for job tracking. Connection ID passed in request, results pushed immediately via WebSocket Management API.

### Current Problem

The current execution flow is **synchronous request-response**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚â”€â”€â”€â”€â–¶â”‚ API Gateway â”‚â”€â”€â”€â”€â–¶â”‚   Lambda    â”‚â”€â”€â”€â”€â–¶â”‚ Execute (in-proc) â”‚
â”‚          â”‚â—€â”€â”€â”€â”€â”‚             â”‚â—€â”€â”€â”€â”€â”‚  (Mangum)   â”‚â—€â”€â”€â”€â”€â”‚ or Fargate task   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–²                                         â”‚
                        â”‚                                         â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€ waits synchronously â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Problems:**

| Issue | Impact |
|-------|--------|
| **API Gateway timeout (29s)** | If execution + Fargate cold start > 29s â†’ client gets 504 |
| **Lambda timeout billing** | Lambda sits idle while polling Fargate |
| **No retry on failures** | Transient errors fail the entire request |
| **No backpressure** | Traffic spike = uncontrolled Fargate task explosion |
| **Poor UX** | User stares at spinner with no progress feedback |
| **No execution history** | Results lost after response sent |

### Proposed Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Async Execution with SQS                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Frontend â”‚â”€â”€â”€â–¶â”‚ POST /exec  â”‚â”€â”€â”€â–¶â”‚ SQS Queue   â”‚â”€â”€â”€â–¶â”‚  Worker Lambda      â”‚ â”‚
â”‚  â”‚          â”‚    â”‚ (returns    â”‚    â”‚ (jobs)      â”‚    â”‚  (or Fargate)       â”‚ â”‚
â”‚  â”‚          â”‚    â”‚  job_id)    â”‚    â”‚             â”‚    â”‚                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                                                           â”‚            â”‚
â”‚       â”‚ poll or WebSocket                                         â”‚ writes     â”‚
â”‚       â”‚                                                           â–¼            â”‚
â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ GET /status â”‚â—€â”€â”€â”€â”‚  DynamoDB (jobs table)                â”‚   â”‚
â”‚                 â”‚ or WS push  â”‚    â”‚  - job_id, status, created_at         â”‚   â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - result (stdout, stderr, error)     â”‚   â”‚
â”‚                                    â”‚  - TTL for auto-cleanup               â”‚   â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Dead Letter Queue (DLQ)                                                â”‚   â”‚
â”‚  â”‚  - Failed jobs after 3 retries                                          â”‚   â”‚
â”‚  â”‚  - Alarm triggers on DLQ depth                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits

| Benefit | Description |
|---------|-------------|
| **Decoupled** | API responds immediately with `job_id` (< 100ms), no timeout risk |
| **Retries** | SQS handles retries with exponential backoff (3 attempts default) |
| **DLQ** | Failed executions go to dead-letter queue for debugging/alerting |
| **Throttling** | Control concurrency via Lambda reserved concurrency |
| **Progress** | Can update status: `pending` â†’ `running` â†’ `completed` / `failed` |
| **History** | Results stored in DynamoDB, queryable later |
| **WebSockets** | Push results to frontend when ready (optional enhancement) |
| **Reusable** | Same pattern for snippet sync, LLM analysis, etc. |

### Data Model (DynamoDB)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jobs Table                                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  Partition Key: job_id (UUID)                                                   â”‚
â”‚  Sort Key: (none - single item per job)                                         â”‚
â”‚                                                                                 â”‚
â”‚  Attributes:                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  job_id:        UUID (PK)                                               â”‚   â”‚
â”‚  â”‚  user_id:       string (Cognito sub, GSI for user's jobs)               â”‚   â”‚
â”‚  â”‚  status:        enum (pending | running | completed | failed)           â”‚   â”‚
â”‚  â”‚  code:          string (the submitted code)                             â”‚   â”‚
â”‚  â”‚  timeout_seconds: number                                                â”‚   â”‚
â”‚  â”‚                                                                         â”‚   â”‚
â”‚  â”‚  # Set when running                                                     â”‚   â”‚
â”‚  â”‚  started_at:    ISO timestamp                                           â”‚   â”‚
â”‚  â”‚                                                                         â”‚   â”‚
â”‚  â”‚  # Set when completed/failed                                            â”‚   â”‚
â”‚  â”‚  completed_at:  ISO timestamp                                           â”‚   â”‚
â”‚  â”‚  result: {                                                              â”‚   â”‚
â”‚  â”‚    success:     boolean                                                 â”‚   â”‚
â”‚  â”‚    stdout:      string                                                  â”‚   â”‚
â”‚  â”‚    stderr:      string                                                  â”‚   â”‚
â”‚  â”‚    error:       string | null                                           â”‚   â”‚
â”‚  â”‚    error_type:  string | null                                           â”‚   â”‚
â”‚  â”‚    execution_time_ms: number                                            â”‚   â”‚
â”‚  â”‚    timed_out:   boolean                                                 â”‚   â”‚
â”‚  â”‚  }                                                                      â”‚   â”‚
â”‚  â”‚                                                                         â”‚   â”‚
â”‚  â”‚  # Metadata                                                             â”‚   â”‚
â”‚  â”‚  created_at:    ISO timestamp                                           â”‚   â”‚
â”‚  â”‚  ttl:           Unix timestamp (auto-delete after 24h)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                 â”‚
â”‚  GSI: user_id-created_at-index (for listing user's recent jobs)                â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Changes

**New Endpoints:**

```python
# POST /execute â†’ Returns job_id immediately
@router.post("/execute", response_model=JobSubmittedResponse)
async def submit_execution(
    request: ExecutionRequest,
    user: User = Depends(get_current_user),
    job_service: JobService = Depends(get_job_service),
) -> JobSubmittedResponse:
    """Submit code for async execution. Returns job_id to poll for results."""
    job = await job_service.submit(
        user_id=user.sub,
        code=request.code,
        timeout_seconds=request.timeout_seconds,
    )
    return JobSubmittedResponse(
        job_id=job.job_id,
        status="pending",
        poll_url=f"/jobs/{job.job_id}",
    )


# GET /jobs/{job_id} â†’ Poll for status/results
@router.get("/jobs/{job_id}", response_model=JobStatusResponse)
async def get_job_status(
    job_id: str,
    user: User = Depends(get_current_user),
    job_service: JobService = Depends(get_job_service),
) -> JobStatusResponse:
    """Get the status and results of an execution job."""
    job = await job_service.get(job_id, user_id=user.sub)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    return JobStatusResponse(
        job_id=job.job_id,
        status=job.status,
        result=job.result if job.status in ("completed", "failed") else None,
        created_at=job.created_at,
        started_at=job.started_at,
        completed_at=job.completed_at,
    )


# GET /jobs â†’ List user's recent jobs
@router.get("/jobs", response_model=list[JobSummary])
async def list_jobs(
    user: User = Depends(get_current_user),
    job_service: JobService = Depends(get_job_service),
    limit: int = Query(default=10, le=50),
) -> list[JobSummary]:
    """List the user's recent execution jobs."""
    return await job_service.list_for_user(user_id=user.sub, limit=limit)
```

**Response Models:**

```python
# api/schemas/jobs.py

class JobSubmittedResponse(BaseModel):
    job_id: str
    status: Literal["pending"]
    poll_url: str


class JobResult(BaseModel):
    success: bool
    stdout: str | None = None
    stderr: str | None = None
    error: str | None = None
    error_type: str | None = None
    execution_time_ms: float | None = None
    timed_out: bool = False


class JobStatusResponse(BaseModel):
    job_id: str
    status: Literal["pending", "running", "completed", "failed"]
    result: JobResult | None = None
    created_at: datetime
    started_at: datetime | None = None
    completed_at: datetime | None = None


class JobSummary(BaseModel):
    job_id: str
    status: str
    created_at: datetime
    execution_time_ms: float | None = None
```

### Job Service Implementation

```python
# backend/api/services/job_service.py

import json
import uuid
from datetime import datetime, timedelta

import boto3

from api.schemas.jobs import JobResult, JobStatusResponse
from common.config import settings


class JobService:
    """Service for managing async execution jobs."""

    def __init__(self):
        self.sqs = boto3.client("sqs", region_name=settings.aws_region)
        self.dynamodb = boto3.resource("dynamodb", region_name=settings.aws_region)
        self.jobs_table = self.dynamodb.Table(settings.jobs_table_name)
        self.queue_url = settings.execution_queue_url

    async def submit(
        self,
        user_id: str,
        code: str,
        timeout_seconds: float | None = None,
    ) -> JobStatusResponse:
        """Submit a new execution job."""
        job_id = str(uuid.uuid4())
        now = datetime.utcnow()
        ttl = int((now + timedelta(hours=24)).timestamp())

        # 1. Create job record in DynamoDB (pending)
        item = {
            "job_id": job_id,
            "user_id": user_id,
            "status": "pending",
            "code": code,
            "timeout_seconds": timeout_seconds or settings.execution_timeout_seconds,
            "created_at": now.isoformat(),
            "ttl": ttl,
        }
        self.jobs_table.put_item(Item=item)

        # 2. Send message to SQS queue
        self.sqs.send_message(
            QueueUrl=self.queue_url,
            MessageBody=json.dumps({
                "job_id": job_id,
                "user_id": user_id,
                "code": code,
                "timeout_seconds": item["timeout_seconds"],
            }),
            MessageGroupId=user_id,  # FIFO: preserve order per user
            MessageDeduplicationId=job_id,
        )

        return JobStatusResponse(
            job_id=job_id,
            status="pending",
            created_at=now,
        )

    async def get(self, job_id: str, user_id: str) -> JobStatusResponse | None:
        """Get a job by ID, validating ownership."""
        response = self.jobs_table.get_item(Key={"job_id": job_id})
        item = response.get("Item")
        
        if not item or item.get("user_id") != user_id:
            return None

        return self._item_to_response(item)

    async def update_status(
        self,
        job_id: str,
        status: str,
        result: JobResult | None = None,
    ) -> None:
        """Update job status (called by worker)."""
        now = datetime.utcnow().isoformat()
        
        update_expr = "SET #status = :status"
        expr_values = {":status": status}
        expr_names = {"#status": "status"}

        if status == "running":
            update_expr += ", started_at = :started_at"
            expr_values[":started_at"] = now
        elif status in ("completed", "failed"):
            update_expr += ", completed_at = :completed_at"
            expr_values[":completed_at"] = now
            if result:
                update_expr += ", #result = :result"
                expr_names["#result"] = "result"
                expr_values[":result"] = result.model_dump()

        self.jobs_table.update_item(
            Key={"job_id": job_id},
            UpdateExpression=update_expr,
            ExpressionAttributeNames=expr_names,
            ExpressionAttributeValues=expr_values,
        )

    def _item_to_response(self, item: dict) -> JobStatusResponse:
        """Convert DynamoDB item to response model."""
        result = None
        if "result" in item:
            result = JobResult(**item["result"])
        
        return JobStatusResponse(
            job_id=item["job_id"],
            status=item["status"],
            result=result,
            created_at=datetime.fromisoformat(item["created_at"]),
            started_at=datetime.fromisoformat(item["started_at"]) if item.get("started_at") else None,
            completed_at=datetime.fromisoformat(item["completed_at"]) if item.get("completed_at") else None,
        )
```

### Worker Lambda (SQS Consumer)

```python
# backend/api/handlers/execution_worker.py

"""
SQS-triggered Lambda that processes execution jobs.

This worker:
1. Receives job from SQS queue
2. Updates status to "running" in DynamoDB
3. Executes the code (in-process or via Fargate)
4. Updates status to "completed" or "failed" with results
"""

import json
import logging

from api.schemas.jobs import JobResult
from api.services.job_service import JobService
from api.services.lambda_executor import LambdaExecutor

logger = logging.getLogger()
logger.setLevel(logging.INFO)

job_service = JobService()
executor = LambdaExecutor()


def handler(event, context):
    """Process SQS messages containing execution jobs."""
    
    for record in event.get("Records", []):
        process_job(record)
    
    # Return success - failed messages will be retried by SQS
    return {"statusCode": 200}


def process_job(record: dict) -> None:
    """Process a single execution job from SQS."""
    try:
        body = json.loads(record["body"])
        job_id = body["job_id"]
        code = body["code"]
        timeout_seconds = body.get("timeout_seconds", 30)
        
        logger.info(f"Processing job {job_id}")
        
        # 1. Mark job as running
        job_service.update_status(job_id, "running")
        
        # 2. Execute the code
        result = executor.execute(code, timeout_seconds=timeout_seconds)
        
        # 3. Convert to JobResult
        job_result = JobResult(
            success=result.success,
            stdout=result.stdout,
            stderr=result.stderr,
            error=result.error,
            error_type=result.error_type,
            execution_time_ms=result.execution_time_ms,
            timed_out=result.timed_out,
        )
        
        # 4. Update job as completed/failed
        status = "completed" if result.success else "failed"
        job_service.update_status(job_id, status, result=job_result)
        
        logger.info(f"Job {job_id} {status}")
        
    except Exception as e:
        logger.error(f"Error processing job: {e}")
        # Don't catch - let SQS retry
        raise
```

### Infrastructure (Pulumi)

```python
# infra/pulumi/components/execution_queue.py

import json
import pulumi
import pulumi_aws as aws


def create_execution_queue(env: str, worker_lambda: aws.lambda_.Function):
    """Create SQS queue for async execution jobs."""
    
    # Dead Letter Queue for failed jobs
    dlq = aws.sqs.Queue(
        f"{env}-execution-dlq",
        name=f"code-remote-{env}-execution-dlq.fifo",
        fifo_queue=True,
        message_retention_seconds=1209600,  # 14 days
    )
    
    # Main execution queue
    queue = aws.sqs.Queue(
        f"{env}-execution-queue",
        name=f"code-remote-{env}-execution.fifo",
        fifo_queue=True,
        content_based_deduplication=False,
        visibility_timeout_seconds=60,  # Must be > Lambda timeout
        receive_wait_time_seconds=20,  # Long polling
        redrive_policy=dlq.arn.apply(lambda arn: json.dumps({
            "deadLetterTargetArn": arn,
            "maxReceiveCount": 3,  # 3 retries before DLQ
        })),
    )
    
    # Lambda trigger from SQS
    aws.lambda_.EventSourceMapping(
        f"{env}-execution-trigger",
        event_source_arn=queue.arn,
        function_name=worker_lambda.name,
        batch_size=1,  # Process one job at a time
        enabled=True,
    )
    
    # CloudWatch alarm for DLQ depth
    aws.cloudwatch.MetricAlarm(
        f"{env}-dlq-alarm",
        alarm_name=f"code-remote-{env}-execution-dlq-depth",
        comparison_operator="GreaterThanThreshold",
        evaluation_periods=1,
        metric_name="ApproximateNumberOfMessagesVisible",
        namespace="AWS/SQS",
        period=300,
        statistic="Sum",
        threshold=0,
        alarm_description="Execution jobs failing and going to DLQ",
        dimensions={"QueueName": dlq.name},
        # alarm_actions=[sns_topic.arn],  # Add SNS for notifications
    )
    
    return queue, dlq


def create_jobs_table(env: str):
    """Create DynamoDB table for job status tracking."""
    
    table = aws.dynamodb.Table(
        f"{env}-jobs-table",
        name=f"code-remote-{env}-jobs",
        billing_mode="PAY_PER_REQUEST",  # Serverless pricing
        hash_key="job_id",
        attributes=[
            {"name": "job_id", "type": "S"},
            {"name": "user_id", "type": "S"},
            {"name": "created_at", "type": "S"},
        ],
        global_secondary_indexes=[
            {
                "name": "user_id-created_at-index",
                "hash_key": "user_id",
                "range_key": "created_at",
                "projection_type": "ALL",
            },
        ],
        ttl={
            "attribute_name": "ttl",
            "enabled": True,
        },
    )
    
    return table
```

### Frontend Changes

```typescript
// frontend/src/hooks/useExecution.ts

interface Job {
  job_id: string;
  status: 'pending' | 'running' | 'completed' | 'failed';
  result?: {
    success: boolean;
    stdout?: string;
    stderr?: string;
    error?: string;
    execution_time_ms?: number;
    timed_out: boolean;
  };
}

export function useExecution() {
  const [job, setJob] = useState<Job | null>(null);
  const [isPolling, setIsPolling] = useState(false);

  const execute = async (code: string, timeout?: number) => {
    // 1. Submit job
    const response = await api.post('/execute', { code, timeout_seconds: timeout });
    const { job_id } = response.data;
    
    setJob({ job_id, status: 'pending' });
    setIsPolling(true);
    
    // 2. Poll for results
    const pollInterval = setInterval(async () => {
      const statusResponse = await api.get(`/jobs/${job_id}`);
      const updatedJob = statusResponse.data;
      
      setJob(updatedJob);
      
      if (updatedJob.status === 'completed' || updatedJob.status === 'failed') {
        clearInterval(pollInterval);
        setIsPolling(false);
      }
    }, 1000); // Poll every second
    
    // Cleanup on unmount
    return () => clearInterval(pollInterval);
  };

  return { job, isPolling, execute };
}
```

### Event Flow Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Async Execution Flow                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  1. User clicks "Run"                                                         â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â–¼                                                                         â”‚
â”‚  2. POST /execute                                                             â”‚
â”‚     â”‚  â”œâ”€â–¶ Create job in DynamoDB (status: pending)                          â”‚
â”‚     â”‚  â””â”€â–¶ Send message to SQS queue                                         â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â–¼                                                                         â”‚
â”‚  3. Return { job_id, status: "pending" } immediately (< 100ms)               â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â–¼                                                                         â”‚
â”‚  4. Frontend starts polling GET /jobs/{job_id}                               â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  â”‚  Meanwhile, in the background...                                â”‚   â”‚
â”‚     â”‚  â”‚                                                                 â”‚   â”‚
â”‚     â”‚  â”‚  5. SQS triggers Worker Lambda                                  â”‚   â”‚
â”‚     â”‚  â”‚     â”‚                                                           â”‚   â”‚
â”‚     â”‚  â”‚     â–¼                                                           â”‚   â”‚
â”‚     â”‚  â”‚  6. Worker updates job status to "running"                      â”‚   â”‚
â”‚     â”‚  â”‚     â”‚                                                           â”‚   â”‚
â”‚     â”‚  â”‚     â–¼                                                           â”‚   â”‚
â”‚     â”‚  â”‚  7. Worker executes code (sandbox)                              â”‚   â”‚
â”‚     â”‚  â”‚     â”‚                                                           â”‚   â”‚
â”‚     â”‚  â”‚     â–¼                                                           â”‚   â”‚
â”‚     â”‚  â”‚  8. Worker updates job status to "completed" with result        â”‚   â”‚
â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â–¼                                                                         â”‚
â”‚  9. Frontend poll receives { status: "completed", result: {...} }            â”‚
â”‚     â”‚                                                                         â”‚
â”‚     â–¼                                                                         â”‚
â”‚  10. Display results to user                                                  â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Migration Strategy

Since execution is currently synchronous, we need a migration path:

| Phase | Action |
|-------|--------|
| **10.1** | Deploy DynamoDB table + SQS queue (Pulumi) |
| **10.2** | Deploy worker Lambda with SQS trigger |
| **10.3** | Add new `/execute` (async) + `/jobs/{id}` endpoints |
| **10.4** | Update frontend to use async flow |
| **10.5** | Keep old sync endpoint as `/execute/sync` (deprecated) |
| **10.6** | Remove sync endpoint after verification |

### Cost Estimate

| Component | Monthly Cost (10K executions) |
|-----------|-------------------------------|
| SQS FIFO | ~$0.50 (10K messages) |
| DynamoDB | ~$1.00 (on-demand, with TTL) |
| Worker Lambda | ~$2.00 (10K invocations Ã— 30s avg) |
| CloudWatch | ~$0.30 (alarms) |
| **Total** | **~$4/mo** |

### Future Enhancements

| Enhancement | Description |
|-------------|-------------|
| **WebSockets** | Push results instead of polling (API Gateway WebSocket) |
| **Progress streaming** | Stream stdout in real-time via WebSocket |
| **Priority queues** | Separate queues for free/paid users |
| **Batch execution** | Run multiple code blocks in sequence |
| **Scheduled execution** | Cron-like scheduled jobs |

### Implementation Steps (Phase 10)

| Step | Task | Details |
|------|------|---------|
| 10.1 | Pulumi: SQS FIFO queue + DLQ | With CloudWatch alarm |
| 10.2 | Pulumi: DynamoDB jobs table | With TTL and GSI |
| 10.3 | Worker Lambda | SQS consumer with executor |
| 10.4 | JobService | Submit, get, update_status |
| 10.5 | New API endpoints | POST /execute, GET /jobs/{id}, GET /jobs |
| 10.6 | Frontend: useExecution hook | Submit + poll pattern |
| 10.7 | Frontend: Job status UI | Pending/running/completed states |
| 10.8 | Remove sync execution | Deprecate old endpoint |

**Status:** âœ… COMPLETE (Commit: ec58019)

**Implementation Notes:**
- Stateless design (no DynamoDB jobs table)
- SQS FIFO + Worker Lambda + WebSocket push
- Local dev fallback with FastAPI WebSocket
- Connection status indicator in editor status bar
